{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGo-MwA1PU81"
      },
      "source": [
        "### Rappel Google Colab\n",
        "\n",
        "Tout d'abord, sélectionnez l'option GPU de Colab avec *Edit > Notebook settings* et sélectionner GPU comme Hardware accelerator.\n",
        "Installer ensuite deeplib avec la commande suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6ZA3NS7wPU82",
        "outputId": "9b411ac3-c139-4b34-91ea-34bd3699bf29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ulaval-damas/glo4030-labs.git\n",
            "  Cloning https://github.com/ulaval-damas/glo4030-labs.git to /tmp/pip-req-build-xblr3g6e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ulaval-damas/glo4030-labs.git /tmp/pip-req-build-xblr3g6e\n",
            "  Resolved https://github.com/ulaval-damas/glo4030-labs.git to commit c15d28e9050701a5555ed3f2f1421840cde5e113\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (0.24.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (2.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (6.17.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (1.6.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (0.21)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (7.34.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (11.3.0)\n",
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (1.17.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (0.8.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (from deeplib==0.1) (4.57.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim->deeplib==0.1) (7.5.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->deeplib==0.1) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->deeplib==0.1) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->deeplib==0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->deeplib==0.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->deeplib==0.1) (2025.3)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from poutyne->deeplib==0.1) (1.8.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->deeplib==0.1) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->deeplib==0.1) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->deeplib==0.1) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]->deeplib==0.1) (1.12.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]->deeplib==0.1) (1.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->deeplib==0.1) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeplib==0.1) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeplib==0.1) (5.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->deeplib==0.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->deeplib==0.1) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->deeplib==0.1) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim->deeplib==0.1) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->deeplib==0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->deeplib==0.1) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]->deeplib==0.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]->deeplib==0.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]->deeplib==0.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]->deeplib==0.1) (2026.1.4)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics->poutyne->deeplib==0.1) (0.15.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->deeplib==0.1) (4.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ulaval-damas/glo4030-labs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDjJn3sPU82"
      },
      "source": [
        "# Laboratoire 2: Graphe computationnel, backprop et fonctions d'activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DhCfuBhTPU82"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# %matplotlib inline\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from deeplib.visualization import make_vizualization_autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uIYfMUKPU82"
      },
      "source": [
        "##  Partie 1: Graphe computationnel et backprop\n",
        "Cette section a pour but de vous familiariser avec les notions de graphe computationnel et de backpropagation, plus particulièrement leur implémentation PyTorch. Dans le dernier laboratoire, vous avez vu une version haut-niveau de l'entraînement de réseaux de neurones. À l'inverse, ce laboratoire a pour but de vous donner une intuition du fonctionnement interne de PyTorch.\n",
        "### Tenseurs\n",
        "La structure de données de base dans PyTorch est le `Tensor`. Cette structure de données est comparable au `ndarray` numpy, avec l'avantage que les calculs matriciels peuvent être effectués sur GPU. Le package `torch.Tensor` définit des matrices multidimensionnelles et les opérations sur celles-ci. Voici quelques exemples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXdCp5vjPU82"
      },
      "source": [
        "On peut créer un nouveau `Tensor` de dimension $3 \\times 3$ sans initialisation avec `torch.Tensor()` ou `torch.empty()`. Cependant, les valeurs déjà présentes dans la mémoire allouée sont utilisées comme valeurs initiales et peuvent donc contenir des NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-2PUqPHnPU82",
        "outputId": "fc1e5405-6346-4e1e-ccc5-c5343e902248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.3895e+30, 4.3754e-41, 2.5129e-16],\n",
            "        [0.0000e+00, 4.4842e-44, 0.0000e+00],\n",
            "        [1.7937e-43, 0.0000e+00, 2.8434e-40]])\n",
            "tensor([[4.3895e+30, 4.3754e-41, 4.3895e+30],\n",
            "        [4.3754e-41, 4.4842e-44, 0.0000e+00],\n",
            "        [1.7937e-43, 0.0000e+00, 6.7262e-44]])\n"
          ]
        }
      ],
      "source": [
        "print(torch.Tensor(3, 3))\n",
        "print(torch.empty(3, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jFsPEkyPU82"
      },
      "source": [
        "Il est possible d'initialiser un `Tensor` avec des valeurs plus utiles, comme par exemple avec des valeurs de 0 ou 1, ou encore pour obtenir une matrice identité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EZ4ozt_APU83",
        "outputId": "1c0719fc-71a6-4b77-d667-5a9aa59e121b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "print(torch.zeros(3, 3))\n",
        "print(torch.ones(3, 3))\n",
        "print(torch.eye(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1fR6Pj6PU83"
      },
      "source": [
        "On peut également créer un `Tensor` à partir de données existantes fournies explicitement ou à partir de structures de `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9LE385TePU83",
        "outputId": "dbd87997-d0b7-4453-d77a-f2bf6a27a773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "[0.1  0.25 0.5 ] <class 'numpy.ndarray'>\n",
            "tensor([0.1000, 0.2500, 0.5000], dtype=torch.float64) <class 'torch.Tensor'>\n",
            "[0.1  0.25 0.5 ] <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(torch.tensor([1, 2, 3]))\n",
        "a = np.array([0.1, 0.25, 0.5])\n",
        "print(a, type(a))\n",
        "a = torch.from_numpy(a)\n",
        "print(a, type(a))\n",
        "a = a.numpy()\n",
        "print(a, type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bBPiUWyPU83"
      },
      "source": [
        "Il est souvent nécessaire d'initialiser un `Tensor` avec des valeurs aléatoires, comme par exemple selon une normale centrée à 0 et de variance 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LlsS0gVvPU83",
        "outputId": "fb1256e8-1570-4ae7-d26c-a8c049bc31c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2942,  0.9310,  1.4744],\n",
            "        [ 1.6938, -0.4168, -0.3857],\n",
            "        [ 0.2746,  0.2363,  2.0982]])\n",
            "tensor(0.5124)\n",
            "tensor(1.2597)\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(3, 3)\n",
        "print(a)\n",
        "print(torch.mean(a))\n",
        "print(torch.var(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxMvDYyzPU83"
      },
      "source": [
        "On peut également d'initialiser les valeurs d'un `Tensor` déjà existant avec des fonctions comme `normal_()` et `fill_()`. À noter, une méthode se terminant par un trait de soulignement (underscore) signifie que cette méthode fait une mutation du `Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lHpv01syPU83",
        "outputId": "1e25ec11-10ab-491a-df91-3698b49dc95c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6.6861e-10, 0.0000e+00, 9.9584e-15],\n",
            "        [0.0000e+00, 8.9683e-44, 0.0000e+00],\n",
            "        [1.3452e-43, 0.0000e+00, 2.8433e-40]])\n",
            "tensor([[1.3300e-15],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00]])\n",
            "tensor([[-1.3470, -0.7695, -0.4270],\n",
            "        [ 1.8249, -0.3513,  0.1825],\n",
            "        [-0.8036, -0.9542,  1.2309]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "a, b = torch.empty(3, 3), torch.empty(3, 1)\n",
        "print(a)\n",
        "print(b)\n",
        "a.normal_()\n",
        "b.fill_(1)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vsmj11PU83"
      },
      "source": [
        "Les opérations matricielles s'effectuent naturellement sur les `Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AIsh19XGPU83",
        "outputId": "06788366-6536-4987-93d5-0384713e5b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.5435],\n",
            "        [ 1.6561],\n",
            "        [-0.5269]])\n",
            "tensor([[-2.5435],\n",
            "        [ 1.6561],\n",
            "        [-0.5269]])\n"
          ]
        }
      ],
      "source": [
        "print(a.matmul(b))\n",
        "print(torch.matmul(a, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUjzaDOQPU83"
      },
      "source": [
        "On peut facilement transférer un `Tensor` sur GPU (*Attention: une erreur est soulevée si on tente cette opération sur un ordinateur qui ne possède pas de carte graphique compatible avec les installations requises comme CUDA*). Les opérations sur ces `Tensor` seront alors exécutées sur GPU. On utilise la fonction [`.cuda()`](https://pytorch.org/docs/stable/tensors.html?highlight=cuda#torch.Tensor.cuda) pour envoyer un `Tensor` sur GPU et la fonction [`.cpu()`](https://pytorch.org/docs/stable/tensors.html?highlight=cpu#torch.Tensor.cpu) pour rapporter un `Tensor` vers le CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eHAxb9NwPU83",
        "outputId": "93a33cda-d0c3-4fdb-9d90-44570f76b78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.5435],\n",
            "        [ 1.6561],\n",
            "        [-0.5269]], device='cuda:0')\n",
            "tensor([[-2.5435],\n",
            "        [ 1.6561],\n",
            "        [-0.5269]])\n"
          ]
        }
      ],
      "source": [
        "a_gpu = a.cuda()\n",
        "b_gpu = b.cuda()\n",
        "print(a_gpu.matmul(b_gpu))\n",
        "print(a_gpu.matmul(b_gpu).cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgzyw2q7PU83"
      },
      "source": [
        "##### Exercice\n",
        " - Corrigez la deuxième opération pour multiplier `a` avec `c_gpu` sur CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m-aZRsJDPU83",
        "outputId": "3fdede5a-751d-4a70-86f6-f10e40d1eea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.3768],\n",
            "        [-5.3194],\n",
            "        [-0.1849]])\n"
          ]
        }
      ],
      "source": [
        "c_gpu = a_gpu.matmul(b_gpu).cpu()\n",
        "# TODO Corrigez le code suivant\n",
        "print(a.matmul(c_gpu))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfKmJkgPU83"
      },
      "source": [
        "### Gradient et rétropropagation (backpropagation)\n",
        "\n",
        "Grâce au package `torch.autograd`, il est possible d'automatiquement calculer la dérivée de fonctions calculées à partir d'opérations sur les tenseurs. PyTorch construit dynamiquement un graphe de calcul indiquant les liens de dépendance entre les tenseurs et les opérations, ce qui permet la backpropagation.\n",
        "\n",
        "> **NOTE** Contrairement à des librairies comme Tensorflow où le graphe de calcul est statique, PyTorch recrée dynamique le graphe de calcul à chaque itération. Cela permet de modifier la structure du graphe dynamiquement avec du code Python. Par contre, cela rend la visualisation du graphe plus difficile.\n",
        "\n",
        "\n",
        "On indique les tenseurs qu'on veut dériver avec `requires_grad=True` (par défaut à False) lors de l'initialisation,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "srZ2vwAQPU83"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(3., requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUFrlZXcPU83"
      },
      "source": [
        "ou encore en modifiant l'attribut `requires_grad` d'un `Tensor` existant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aXBN2vluPU83",
        "outputId": "371923b9-0a69-40ee-9101-7b4fafa430e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x.requires_grad = False\n",
        "print(x)\n",
        "x.requires_grad = True\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vje-NS2kPU83"
      },
      "source": [
        "Le graphe de calcul est alors bâti au fur et à mesure que des opérations mathématiques sont appliquées aux tenseurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "re9slzy6PU83"
      },
      "outputs": [],
      "source": [
        "f = x ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44vnIkXBPU84"
      },
      "source": [
        "Chaque `Tensor` résultant d'une opération comporte un attribut `grad_fn` qui réfère à la `Fonction` (opération) qui l'a créé. Dans notre exemple nous avons utilisé une fonction puissance, par conséquent `grad_fn` de $f$ réfère à `PowBackward` correspondant à la fonction à utiliser pour la backprop (`Backward`) associée à la fonction puissance (`Pow` pour Power)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "p4noymAoPU84",
        "outputId": "24821c76-50e4-45df-cf9c-23a00e34758d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsExxcskPU84"
      },
      "source": [
        "On obtient un graphe acyclique qui encode l'historique de calcul comportant les `Tensor` avec `requires_grad=True` et les `Fonction` appliqués sur ceux-ci. En utilisant la fonction `make_vizualization_autograd` de deeplib, on peut visualiser ce graphe où les `Fonction` sont représentées par un rectangle gris avec la valeur de `grad_fn` et les `Tensor` aillant `requires_grad=True` par un rectangle bleu comportant l'identifiant *Var* ainsi que leur attribut `size`. Dans cet exemple, le rectangle bleu réfère au `Tensor` $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1xCGGNbIPU84",
        "outputId": "59326392-3459-4812-ba68-ddf137c4be39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"101pt\" height=\"99pt\"\n viewBox=\"0.00 0.00 101.00 99.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 95)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-95 97,-95 97,4 -4,4\"/>\n<!-- 134103181089264 -->\n<g id=\"node1\" class=\"node\">\n<title>134103181089264</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"93,-91 0,-91 0,-70 93,-70 93,-91\"/>\n<text text-anchor=\"middle\" x=\"46.5\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\">PowBackward0</text>\n</g>\n<!-- 134103181089408 -->\n<g id=\"node2\" class=\"node\">\n<title>134103181089408</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"73.5,-34 19.5,-34 19.5,0 73.5,0 73.5,-34\"/>\n<text text-anchor=\"middle\" x=\"46.5\" y=\"-20.4\" font-family=\"Times,serif\" font-size=\"12.00\">Var</text>\n<text text-anchor=\"middle\" x=\"46.5\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\"> ()</text>\n</g>\n<!-- 134103181089264&#45;&gt;134103181089408 -->\n<g id=\"edge1\" class=\"edge\">\n<title>134103181089264&#45;&gt;134103181089408</title>\n<path fill=\"none\" stroke=\"black\" d=\"M46.5,-69.89C46.5,-63.09 46.5,-53.62 46.5,-44.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"50,-44.48 46.5,-34.48 43,-44.48 50,-44.48\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x79f754792450>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "make_vizualization_autograd(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stNPWY5qPU84"
      },
      "source": [
        "À la fin des calculs, on appelle la fonction `backward()` qui parcourt le graphe de calcul en sens inverse et calcule le gradient de la fonction $f$ selon chacune des variables du graphe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KI38LMDcPU84"
      },
      "outputs": [],
      "source": [
        "f.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IWWKrIWPU84"
      },
      "source": [
        "Après avoir exécuté la fonction backward(), l'attribut grad des tenseurs impliqués dans le calcul contient la valeur du gradient calculé au point courant. Ici, on aura la valeur :\n",
        "$$\\left[\\frac{\\partial f(x)}{\\partial x}\\right]_{x=3} = \\left[\\frac{\\partial x^2}{\\partial x}\\right]_{x=3} =\\big[\\,2\\,x\\,\\big]_{x=3} = 6$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5wNGHHY5PU84",
        "outputId": "871dab76-714a-436f-d862-0605ca9184ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzl_RI_PU84"
      },
      "source": [
        "À noter, les opérations in-place, c'est-à-dire les opérations qui font une mutation directe d'un ```Tensor``` (et qui se terminent par un underscore), ne sont pas disponibles lorsque ```requires_grad``` est égal à ```True```. Ainsi, l'exécution de la prochaine cellule soulève une erreur:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "sLPUdzHwPU84",
        "outputId": "1ecdc591-9525-483f-f469-57859a3f75a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3468212372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ],
      "source": [
        "x.uniform_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZulwGCPU84"
      },
      "source": [
        "Également, on ne peut pas directement convertir un `Tensor` qui a `requires_grad=True` vers `numpy`, l'exécution de la prochaine cellule soulève donc une erreur:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NaS8yPOdPU84",
        "outputId": "2934ba31-37bf-4bae-9dc1-6f0539b90574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2577979712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ],
      "source": [
        "x.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO7kXsPWPU9A"
      },
      "source": [
        "Il faut d'abord utiliser la fonction `detach()` qui détache sa valeur de l'accumulation de l'historique de calcul."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "To4KivgiPU9B",
        "outputId": "f5e0088d-37bf-40e6-f042-70e6ee875ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(3., dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "x.detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGfXssTMPU9B"
      },
      "source": [
        "Voyons maintenant un exemple plus complexe en effectuant des opérations sur 3 `Tensor` de dimension $2 \\times 2$: $x$, $y$ et $z$. On met `requires_grad=True` seulement pour $y$ et $z$ et on effectue des opérations résultant dans le `Tensor` $f$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XltwWAcMPU9B",
        "outputId": "a8abbe20-27bc-44e0-bc06-7213d4d6a2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            " tensor([[ 0.6611,  0.3415],\n",
            "        [-0.3311,  0.9045]])\n",
            "y:\n",
            " tensor([[ 0.8030, -0.6151],\n",
            "        [-0.7907, -0.6516]], requires_grad=True)\n",
            "z:\n",
            " tensor([[-0.1521, -0.7198],\n",
            "        [-0.1280, -0.5362]], requires_grad=True)\n",
            "f:\n",
            " tensor([[ 1.5727, -1.6225],\n",
            "        [-2.2309, -0.6689]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.Tensor(2, 2).uniform_(-1, 1)\n",
        "x.requires_grad = False\n",
        "print(\"x:\\n\", x)\n",
        "\n",
        "y = torch.Tensor(2, 2).uniform_(-1, 1)\n",
        "y.requires_grad = True\n",
        "print(\"y:\\n\", y)\n",
        "\n",
        "z = torch.Tensor(2, 2).uniform_(-1, 1)\n",
        "z.requires_grad = True\n",
        "print(\"z:\\n\", z)\n",
        "\n",
        "f = torch.matmul(x, y) + x + y + z\n",
        "print(\"f:\\n\", f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MUHcEw0PU9B"
      },
      "source": [
        "Visualisons le graphe de calcul construit par ces opérations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "c9_2rwnVPU9B",
        "outputId": "8cf23558-234e-48ef-a04a-a82e6816ce00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"209pt\" height=\"283pt\"\n viewBox=\"0.00 0.00 209.00 283.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-279 205,-279 205,4 -4,4\"/>\n<!-- 134107678130128 -->\n<g id=\"node1\" class=\"node\">\n<title>134107678130128</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"174,-275 82,-275 82,-254 174,-254 174,-275\"/>\n<text text-anchor=\"middle\" x=\"128\" y=\"-261.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n</g>\n<!-- 134107678129024 -->\n<g id=\"node2\" class=\"node\">\n<title>134107678129024</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"129,-211.5 37,-211.5 37,-190.5 129,-190.5 129,-211.5\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-197.9\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n</g>\n<!-- 134107678130128&#45;&gt;134107678129024 -->\n<g id=\"edge1\" class=\"edge\">\n<title>134107678130128&#45;&gt;134107678129024</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.97,-253.89C114.34,-244.83 104.23,-231.01 96.11,-219.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"98.86,-217.75 90.13,-211.74 93.21,-221.88 98.86,-217.75\"/>\n</g>\n<!-- 134107678128880 -->\n<g id=\"node6\" class=\"node\">\n<title>134107678128880</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"201,-218 147,-218 147,-184 201,-184 201,-218\"/>\n<text text-anchor=\"middle\" x=\"174\" y=\"-204.4\" font-family=\"Times,serif\" font-size=\"12.00\">Var</text>\n<text text-anchor=\"middle\" x=\"174\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n</g>\n<!-- 134107678130128&#45;&gt;134107678128880 -->\n<g id=\"edge6\" class=\"edge\">\n<title>134107678130128&#45;&gt;134107678128880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M135.19,-253.89C140.66,-246.57 148.45,-236.16 155.61,-226.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"158.49,-228.59 161.67,-218.48 152.88,-224.39 158.49,-228.59\"/>\n</g>\n<!-- 134107678128064 -->\n<g id=\"node3\" class=\"node\">\n<title>134107678128064</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"92,-148 0,-148 0,-127 92,-127 92,-148\"/>\n<text text-anchor=\"middle\" x=\"46\" y=\"-134.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddBackward0</text>\n</g>\n<!-- 134107678129024&#45;&gt;134107678128064 -->\n<g id=\"edge2\" class=\"edge\">\n<title>134107678129024&#45;&gt;134107678128064</title>\n<path fill=\"none\" stroke=\"black\" d=\"M77.22,-190.39C71.87,-181.51 63.78,-168.06 57.18,-157.08\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60.02,-155.01 51.86,-148.24 54.02,-158.62 60.02,-155.01\"/>\n</g>\n<!-- 134107678114144 -->\n<g id=\"node5\" class=\"node\">\n<title>134107678114144</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"110,-34 56,-34 56,0 110,0 110,-34\"/>\n<text text-anchor=\"middle\" x=\"83\" y=\"-20.4\" font-family=\"Times,serif\" font-size=\"12.00\">Var</text>\n<text text-anchor=\"middle\" x=\"83\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n</g>\n<!-- 134107678129024&#45;&gt;134107678114144 -->\n<g id=\"edge5\" class=\"edge\">\n<title>134107678129024&#45;&gt;134107678114144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.46,-190.29C91.91,-180.03 98.43,-163.26 101,-148 106.75,-113.81 106.75,-104.19 101,-70 99.51,-61.16 96.7,-51.81 93.73,-43.52\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"96.94,-42.12 90.11,-34.03 90.4,-44.62 96.94,-42.12\"/>\n</g>\n<!-- 134107678120624 -->\n<g id=\"node4\" class=\"node\">\n<title>134107678120624</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"91.5,-91 0.5,-91 0.5,-70 91.5,-70 91.5,-91\"/>\n<text text-anchor=\"middle\" x=\"46\" y=\"-77.4\" font-family=\"Times,serif\" font-size=\"12.00\">MmBackward0</text>\n</g>\n<!-- 134107678128064&#45;&gt;134107678120624 -->\n<g id=\"edge3\" class=\"edge\">\n<title>134107678128064&#45;&gt;134107678120624</title>\n<path fill=\"none\" stroke=\"black\" d=\"M46,-126.92C46,-119.91 46,-110.14 46,-101.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"49.5,-101.34 46,-91.34 42.5,-101.34 49.5,-101.34\"/>\n</g>\n<!-- 134107678120624&#45;&gt;134107678114144 -->\n<g id=\"edge4\" class=\"edge\">\n<title>134107678120624&#45;&gt;134107678114144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51.78,-69.89C56.09,-62.72 62.19,-52.58 67.86,-43.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"70.93,-44.85 73.08,-34.48 64.93,-41.25 70.93,-44.85\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x79f860841a30>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "make_vizualization_autograd(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LaImtv8PU9B"
      },
      "source": [
        "#### Questions\n",
        "- En considérant que `Add` réfère à une addition matricielle et `Mm` à une multiplication matricielle, associez chaque rectangle bleu du graphe au `Tensor` ($x$, $y$ ou $z$) correspondant.\n",
        "\n",
        "- Que devient le graphe si on change `x.requires_grad=True`? Effectuez le changement dans le code pour confirmer votre réponse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c5A-aIqPU9B"
      },
      "source": [
        "Effectuons maintenant la backprop du gradient. Au départ les attributs `grad` des `Tensor` sont vides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "3kgoIeuSPU9B",
        "outputId": "5ad0bbab-4026-4ad7-e511-31a5a807f620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st4y6tDWPU9B"
      },
      "source": [
        "On appele la fonction `backward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "G5Ii5UawPU9B"
      },
      "outputs": [],
      "source": [
        "f.backward(torch.ones(f.size()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKipxzvYPU9B"
      },
      "source": [
        "Et on obtient des valeurs de gradients pour les `Tensor` qui avaient `requires_grad=True`, par conséquent `x.grad` devrait toujours être vide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "scrolled": true,
        "id": "r63rsFN5PU9B",
        "outputId": "c0afd0ae-7e7b-4c9c-e2e3-8a85d2d584ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([[1.3300, 1.3300],\n",
            "        [2.2460, 2.2460]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9KY0MMKPU9B"
      },
      "source": [
        "#### Questions\n",
        "- Exécutez deux fois la cellule qui appelle la fonction .backward(). Qu'arrive-t-il? Pourquoi? Ça casse car on a pas effectué la phase d'update qui vide ensuite le gradient.\n",
        "\n",
        "- Quel `Tensor` auraient requires_grad=False dans le contexte d'entraînement de réseaux de neurones? le tensor contenant les données en entrée du réseau de neurones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZm74Lb4PU9B"
      },
      "source": [
        "Il est possible de prévenir l'enregistrement de l'historique de calcul d'opérations effectuées sur des `Tensor` aillant `requires_grad=True` en utilisant un bloc de code avec `with torch.no_grad()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "AjDtI1AwPU9B",
        "outputId": "b0ba108f-60b3-49f2-d04b-76b10190f753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print((y + z).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((y + z).requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3mrORfNPU9B"
      },
      "source": [
        "#### Exercice\n",
        "- Faites la mise à jour des valeurs de y et z et soustrayant leur gradient multiplié par $1 \\times 10^{-3}$  sans créer un nouveau graphe de calcul lors de l'opération. Notez que si le tenseur résultant a un attribut `grad_fn`, un graphe de calcul a été créé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oyQBpdhpPU9B",
        "outputId": "ace75b58-5a4e-4cfa-f39f-1baa0b355199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.4633, -1.6612],\n",
            "        [-0.9360,  1.0328]])\n",
            "tensor([[ 0.0577,  0.1340],\n",
            "        [-0.7667,  1.1867]])\n"
          ]
        }
      ],
      "source": [
        "y = torch.randn((2, 2), requires_grad=True)\n",
        "z = torch.randn((2, 2), requires_grad=True)\n",
        "\n",
        "f = torch.matmul(y, z)\n",
        "f.backward(torch.ones(f.size()))\n",
        "\n",
        "# TODO Mise à jour de y et z\n",
        "with torch.no_grad():\n",
        "  y = y - y.grad * 1e-3\n",
        "  z = z - z.grad * 1e-3\n",
        "\n",
        "print(y)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG0J72_qPU9B"
      },
      "source": [
        "#### Question\n",
        "- Dans le contexte des réseaux de neurones, dans quelle situation voudrait-on ne calculer aucun gradient d'un graphe de calcul? Lors de l'inférence, c'est-à-dire lors de l'utilisation du réseau plutôt que lors de l'entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvwUOTBaPU9B"
      },
      "source": [
        "### Descente du gradient: un aperçu\n",
        "Considérons une fonction simple en une dimension:\n",
        "$$ f(x) = x^2 - x + 2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Ko_6aBCRPU9B"
      },
      "outputs": [],
      "source": [
        "def fonction_simple(x):\n",
        "    return x**2 - x + 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Naat91scPU9B"
      },
      "source": [
        "On peut visualiser cette fonction convexe qui possède un minimum global en $x = 0.5$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "jXbyA1cuPU9B",
        "outputId": "2fc08aed-a9e9-47e0-c533-4b5b7bc648bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "x_range = np.linspace(-2, 3)\n",
        "plt.plot(x_range, fonction_simple(x_range) )\n",
        "plt.scatter(.5, fonction_simple(.5), s=150, marker='*', c='r')\n",
        "plt.ylim(0, 6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh7ZY8vMPU9B"
      },
      "source": [
        "Nous allons tenter de trouver ce minimum global en partant avec $x = -2$ par la méthode de descente du gradient (vu en détails au prochain cours) en utilisant la dérivation automatique de PyTorch. À chaque itération, la fonction est évaluée au paramètre $x$, le gradient est obtenu en appelant `backward`, puis la valeur du paramètre `x` est mise à jour selon la direction pointée par le gradient et un certain pas (0.25 dans cet exemple)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmsAVuzdPU9C"
      },
      "source": [
        "#### Exercice\n",
        "- Dans son état actuel, le code ne permet pas de converger vers le minimum global. Une étape est manquante dans la boucle de descente du gradient concernant le graphe de calcul. Ajoutez la ligne manquante et confirmez que le paramètre $x$ converge bien vers le minimum global. **Indice: observez les valeurs de f'(x) au fil des itérations. Est-ce que les valeurs correspondent aux gradients attendus?**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CdraizNAPU9C",
        "outputId": "557b6d30-60da-4301-f898-0ddd72fd4ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 01:  x =-2.00000  f(x) = 8.00000  f'(x) =-5.00000\n",
            "Iteration 02:  x =-0.75000  f(x) = 3.31250  f'(x) =-2.50000\n",
            "Iteration 03:  x =-0.12500  f(x) = 2.14062  f'(x) =-1.25000\n",
            "Iteration 04:  x = 0.18750  f(x) = 1.84766  f'(x) =-0.62500\n",
            "Iteration 05:  x = 0.34375  f(x) = 1.77441  f'(x) =-0.31250\n",
            "Iteration 06:  x = 0.42188  f(x) = 1.75610  f'(x) =-0.15625\n",
            "Iteration 07:  x = 0.46094  f(x) = 1.75153  f'(x) =-0.07812\n",
            "Iteration 08:  x = 0.48047  f(x) = 1.75038  f'(x) =-0.03906\n",
            "Iteration 09:  x = 0.49023  f(x) = 1.75010  f'(x) =-0.01953\n",
            "Iteration 10:  x = 0.49512  f(x) = 1.75002  f'(x) =-0.00977\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Nombre d'itérations\n",
        "T = 10\n",
        "\n",
        "# Initialisation\n",
        "x = torch.Tensor([-2])\n",
        "x.requires_grad = True\n",
        "list_x, list_values = [], []\n",
        "\n",
        "# Boucle de descente du gradient\n",
        "for t in range(T):\n",
        "\n",
        "    # Calcul de la fonction objectif\n",
        "    val = fonction_simple(x)\n",
        "\n",
        "    # Enregistrement des valeurs pour visualisation\n",
        "    list_x.append(np.array(x.detach()))\n",
        "    list_values.append(val.detach())\n",
        "\n",
        "    # Calcul des gradients\n",
        "    val.backward()\n",
        "\n",
        "    print(f\"Iteration {t+1:02}:\",\n",
        "          f\" x ={x.item(): .5f}\",\n",
        "          f\" f(x) ={val.item(): .5f}\",\n",
        "          f\" f\\'(x) ={x.grad.item(): .5f}\")\n",
        "\n",
        "    # Mise à jour du paramètre x\n",
        "    with torch.no_grad():\n",
        "        x -= 0.25 * x.grad\n",
        "\n",
        "    # TODO Ajoutez l'étape manquante\n",
        "    x.grad.zero_()\n",
        "\n",
        "plt.plot(x_range, fonction_simple(x_range))\n",
        "plt.plot(list_x, list_values, linestyle='dashed', marker='o', color='green')\n",
        "plt.scatter(.5, fonction_simple(.5), s=150, marker='*', c='r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsQfS03ePU9C"
      },
      "source": [
        "## Partie 2: Fonctions d'activation\n",
        "\n",
        "La section suivante a pour but d'explorer les différences entre les fonctions d'activation ReLU et Tanh.\n",
        "\n",
        "#### Question préalable\n",
        "- À quoi sert la fonction d'activation? Sans elle, que devient un réseau multi-couches?\n",
        "\n",
        "### Visualisation du dataset\n",
        "Pour cette partie, nous utiliserons le dataset binaire des spirales. Chaque exemple est représenté par un vecteur de dimension 7: $[x, y, x^2, y^2, x*y, \\sin(x), \\sin(y)]$, où les deux premières valeurs correspondent aux coordonnées dans un espace en deux dimensions présenté par le graphique suivant, et les autres valeurs sont des transformations sur les deux premières. Vous pouvez voir le code qui a servi à générer le dataset dans la librairie https://github.com/ulaval-damas/glo4030-labs/blob/master/deeplib/datasets.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfwJnVgOPU9C"
      },
      "outputs": [],
      "source": [
        "from deeplib.datasets import SpiralDataset, train_valid_loaders\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "dataset = SpiralDataset()\n",
        "points, labels = dataset.to_numpy()\n",
        "plt.scatter(points[labels==1,0], points[labels==1,1])\n",
        "plt.scatter(points[labels==0,0], points[labels==0,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx2XsaDTPU9C"
      },
      "source": [
        "### Création de modèles\n",
        "\n",
        "Ici, on crée des classes qui héritent de `torch.nn.Module`. C'est la classe de base de tout réseau dans PyTorch. `Module` comporte par exemple la méthode `named_parameters()` qui permet d'obtenir toutes les variables entraînables du `Module` ainsi que leur nom. Voici un lien vers la documentation complète:\n",
        "http://pytorch.org/docs/stable/nn.html#torch.nn.Module.\n",
        "\n",
        "#### Exercice\n",
        "- Complétez la fonction forward de TanhModel et ReluModel en appliquant à chaque couche la transformation linéaire ([Indice](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)) suivie de la fonction activation ([Indice](https://pytorch.org/docs/stable/generated/torch.tanh.html?highlight=tanh#torch.tanh)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIGi5vZ7PU9C"
      },
      "outputs": [],
      "source": [
        "class RandomModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, init_gaussian_mean):\n",
        "        super().__init__()\n",
        "        # Le modèle Tanh et ReLU vont être initialisé avec les mêmes matrices aléatoires\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # Initialisation des couches\n",
        "        self.layers = []\n",
        "        for i in range(n_layers):\n",
        "            layer = nn.Linear(7,7)\n",
        "            layer.weight.data.normal_(init_gaussian_mean, math.sqrt(2 / 7))\n",
        "            layer.bias.data.fill_(0)\n",
        "            self.layers.append(layer)\n",
        "            self.add_module('layer-%d' % i, layer)\n",
        "        self.output_layer = nn.Linear(7,2)\n",
        "\n",
        "        self.nonzero_grad_stats = None\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError('Defined in children classes')\n",
        "\n",
        "    def _forward_output_layer(self, x):\n",
        "        # Couche de sortie avec une neurone par classe\n",
        "        out = self.output_layer(x)\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def compute_gradients_stats(self):\n",
        "        self.nonzero_grad_stats = []\n",
        "\n",
        "        # Calcul pour chaque couche du nombre de poids avec un gradient non nul\n",
        "        # ainsi que la moyenne de ces derniers.\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.weight.grad is not None:\n",
        "                nonzero_grad_indices = torch.nonzero(layer.weight.grad)\n",
        "                nonzero_grad = [layer.weight.grad.data[i,j] for (i,j) in nonzero_grad_indices]\n",
        "                nonzero_grad_mean = np.mean(np.abs(nonzero_grad)) if nonzero_grad else 0\n",
        "                self.nonzero_grad_stats.append((len(nonzero_grad), nonzero_grad_mean))\n",
        "\n",
        "class RandomTanhModel(RandomModel):\n",
        "\n",
        "    def __init__(self, n_layers, init_gaussian_mean=0.0):\n",
        "        super().__init__(n_layers, init_gaussian_mean)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            # TODO\n",
        "\n",
        "        return self._forward_output_layer(out)\n",
        "\n",
        "class RandomReluModel(RandomModel):\n",
        "\n",
        "    def __init__(self, n_layers, init_gaussian_mean=0.0):\n",
        "        super().__init__(n_layers, init_gaussian_mean)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            # TODO\n",
        "\n",
        "        return self._forward_output_layer(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbFJROvPU9C"
      },
      "source": [
        "### Analyse du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QWJa5p3PU9C"
      },
      "outputs": [],
      "source": [
        "# Dataloaders\n",
        "train_loader, valid_loader = train_valid_loaders(dataset, 50)\n",
        "\n",
        "# Paramètres\n",
        "init_gaussian_mean = 0\n",
        "n_layers = 10\n",
        "\n",
        "# Modèles\n",
        "relu_model = RandomReluModel(n_layers, init_gaussian_mean)\n",
        "tanh_model = RandomTanhModel(n_layers, init_gaussian_mean)\n",
        "\n",
        "# Forward pass\n",
        "data, labels = next(iter(train_loader))\n",
        "relu_output = relu_model(data)\n",
        "tanh_output = tanh_model(data)\n",
        "\n",
        "# Calcul de la perte\n",
        "loss = torch.nn.NLLLoss()\n",
        "relu_loss = loss(relu_output, labels)\n",
        "tanh_loss = loss(tanh_output, labels)\n",
        "\n",
        "# Backward\n",
        "relu_loss.backward()\n",
        "tanh_loss.backward()\n",
        "\n",
        "# Calcul des statistiques des gradients\n",
        "relu_model.compute_gradients_stats()\n",
        "tanh_model.compute_gradients_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxSNuIl2PU9C"
      },
      "source": [
        "Le graphique suivant représente en fonction du numéro de la couche à gauche la quantité de poids qui ont un gradient nul lors de la backprop, et à droite le gradient moyen sans tenir compte des gradients nuls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09bAEF_KPU9C"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(14,4))\n",
        "axs[0].plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[0] for x in relu_model.nonzero_grad_stats], label='ReLU')\n",
        "axs[0].plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[0] for x in tanh_model.nonzero_grad_stats], label='Tanh')\n",
        "axs[0].set_ylim([-1, 50])\n",
        "axs[0].set_xlabel('Index de la couche')\n",
        "axs[0].set_ylabel('Nombre de poids avec un gradient non nul')\n",
        "axs[1].plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[1] / x[0] if x[0] else 0 for x in relu_model.nonzero_grad_stats],label='ReLU')\n",
        "axs[1].plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[1] / x[0] if x[0] else 0 for x in tanh_model.nonzero_grad_stats],label='Tanh')\n",
        "axs[1].set_xlabel('Index de la couche')\n",
        "axs[1].set_ylabel('Gradient moyen des gradients non nuls')\n",
        "axs[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3_-Bd1mPU9C"
      },
      "source": [
        "#### Questions\n",
        "- Observez la distribution du gradient lors de la backprop. Quelles différences y a-t-il entre l'utilisation des activations ReLU versus Tanh?\n",
        "\n",
        "- Suite à ces observations, identifiez un problème avec la Relu et identifiez un problème avec la Tanh.\n",
        "\n",
        "- Changez la moyenne de la gaussienne des poids lors de l'initialisation (le paramètre *init_gaussian_mean*). Qu'observez-vous?\n",
        "\n",
        "- Est-ce que, pour deux entrées différentes, les mêmes poids ont un gradient nul?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYyq7I_tPU9C"
      },
      "source": [
        "### Entraînement\n",
        "Pour terminer, on entraîne les deux modèles avec des fonctions d'activation différentes sur le dataset des spirales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsL7WRUaPU9C"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "init_gaussian_mean = 0\n",
        "n_layers = 10\n",
        "n_epoch = 500\n",
        "relu_losses = []\n",
        "tanh_losses = []\n",
        "\n",
        "relu_model = RandomReluModel(n_layers, init_gaussian_mean)\n",
        "tanh_model = RandomTanhModel(n_layers, init_gaussian_mean)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    relu_model.cuda()\n",
        "    tanh_model.cuda()\n",
        "\n",
        "relu_optimizer = SGD(relu_model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
        "tanh_optimizer = SGD(tanh_model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "for epoch in range(1, n_epoch + 1):\n",
        "    if epoch % 100 == 0:\n",
        "        print(\"Epoch %d done.\" % epoch)\n",
        "    relu_epoch_losses = []\n",
        "    tanh_epoch_losses = []\n",
        "    for data, labels in train_loader:\n",
        "        relu_optimizer.zero_grad()\n",
        "        tanh_optimizer.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        relu_loss = loss(relu_model(data), labels)\n",
        "        tanh_loss = loss(tanh_model(data), labels)\n",
        "\n",
        "        relu_loss.backward()\n",
        "        tanh_loss.backward()\n",
        "\n",
        "        relu_epoch_losses.append(float(relu_loss.cpu()))\n",
        "        tanh_epoch_losses.append(float(tanh_loss.cpu()))\n",
        "\n",
        "        relu_optimizer.step()\n",
        "        tanh_optimizer.step()\n",
        "    relu_losses.append(np.mean(np.asarray(relu_epoch_losses)))\n",
        "    tanh_losses.append(np.mean(np.asarray(tanh_epoch_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgFn7ttIPU9C"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(14,4))\n",
        "axs[0].plot(np.arange(len(relu_losses)), np.asarray(relu_losses),label='ReLU', color='tab:blue')\n",
        "axs[1].plot(np.arange(len(tanh_losses)), np.asarray(tanh_losses),label='Tanh', color='tab:orange')\n",
        "for i in range(2):\n",
        "    axs[i].set_xlabel('Epoch')\n",
        "    axs[i].set_ylabel('Loss')\n",
        "    axs[i].set_ylim(0, 1)\n",
        "    axs[i].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8riQSotgPU9C"
      },
      "source": [
        "## Perte d'entropie croisée (Cross-entropy loss) dans PyTorch\n",
        "\n",
        "**Important à savoir lorsqu'on utilise PyTorch pour un problème de classification**: Dans un contexte de tâche de classification, il est d'usage de transformer les sorties réelles d'un réseau de neurones en une distribution de probabilité sur les classes grâce à la fonction [Softmax](https://pytorch.org/docs/master/generated/torch.nn.Softmax.html?#torch.nn.Softmax). Cette distribution peut ensuite être utilisée pour prédire le ou les classes les plus probables selon la donnée en entrée.\n",
        "\n",
        "Voici un exemple de l'utilisation de la fonction ``F.softmax``, vous pouvez modifier les valeurs de sorties pour voir l'impact sur la distribution de probabilité résultante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR2aF6NmPU9C"
      },
      "outputs": [],
      "source": [
        "sorties = torch.tensor([1.7, 1.4, -1.3])\n",
        "F.softmax(sorties, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plZp1d2oPU9C"
      },
      "source": [
        "Afin d'entraîner un réseau de neurones pour une tâche de classification, on utilise habituellement la fonction de perte d'entropie croisée  [CrossEntropyLoss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html?#torch.nn.CrossEntropyLoss) qui combine la fonction de [LogSoftmax](https://pytorch.org/docs/master/generated/torch.nn.LogSoftmax.html?#torch.nn.LogSoftmax) avec la fonction de perte *negative log likelihood* [NLLLoss](https://pytorch.org/docs/master/generated/torch.nn.NLLLoss.html?#torch.nn.NLLLoss). La fonction [LogSoftmax](https://pytorch.org/docs/master/generated/torch.nn.LogSoftmax.html?#torch.nn.LogSoftmax) est utilisée au lieu de la fonction [Softmax](https://pytorch.org/docs/master/generated/torch.nn.Softmax.html?#torch.nn.Softmax) pour des raisons de stabilité numérique et de rapidité de calcul.\n",
        "\n",
        "Pour l'entraînement du modèle plus haut dans ce laboratoire, la fonction de perte d'entropie croisée a été calculée en deux étapes avec ``F.log_softmax`` dans la fonction ``_forward_output_layer(x)`` du ``RandomModel``, suivie du calcul de la perte selon les étiquettes avec la fonction ``loss`` définie par ``torch.nn.NLLLoss()``.\n",
        "\n",
        "Pour utiliser directement ``torch.nn.CrossEntropyLoss()``, le modèle devrait retourner simplement les valeurs réelles de sortie sans appliquer de fonction ``Softmax`` ou ``LogSoftmax``. Le résultat de perte est identique dans les deux cas comme le démontre la prochaine cellule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsYOS2s2PU9C"
      },
      "outputs": [],
      "source": [
        "# Scores d'un réseau de neurones pour 2 exemples d'une tâche à 3 classes avec les étiquettes correspondantes\n",
        "scores = torch.tensor([[1.7, 1.4, -1.3], [0.4, 1.8, 0.9]])\n",
        "labels = torch.tensor([0, 2])\n",
        "\n",
        "# CrossEntropyLoss\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "print(f\"Perte avec CrossEntropyLoss: {loss(scores, labels):.3f}\")\n",
        "\n",
        "# LogSoftmax et NLLLoss\n",
        "loss = torch.nn.NLLLoss()\n",
        "print(f\"Perte avec LogSoftmax et NLLLoss: {loss(F.log_softmax(scores, dim=1), labels):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbj6sS9fPU9C"
      },
      "source": [
        "Dans la librairie deeplib écrite spécialement pour les laboratoires, la fonction ``deeplib.training.train()`` utilise par défaut ``torch.nn.CrossEntropyLoss()``. Par conséquent, les modèles doivent simplement retourner les sorties réelles du réseau de neurones pour pouvoir s'entraîner correctement avec cette fonction ``train()``."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}